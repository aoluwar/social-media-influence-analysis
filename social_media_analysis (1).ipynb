{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7761b1",
   "metadata": {},
   "source": [
    "# Social Media Dataset — EDA & Modeling (Senior DS Workflow)\n",
    "\n",
    "This notebook analyzes the synthetic `social_media_sample.csv` generated earlier.\n",
    "It follows a production-style pipeline:\n",
    "- **EDA**: data quality, distributions, correlations, seasonal trends\n",
    "- **Feature engineering**: time, text, ratios, interactions\n",
    "- **Preprocessing**: `ColumnTransformer` with scaling + one-hot\n",
    "- **Modeling**: compare 6+ models with cross-validation (MAE, RMSE, R²); auto-densify for tree models\n",
    "- **Evaluation**: test metrics, residuals, calibration\n",
    "- **Interpretability**: feature importances for tree models\n",
    "- **Business insights**: posting hour/platform/type patterns\n",
    "\n",
    "> Data path expected at: `/mnt/data/social_media_sample.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ae931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings, math, re, os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Linear (sparse-friendly) + Tree models\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6620483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "PATH = 'data/social_media_sample.csv'\n",
    "df = pd.read_csv(PATH, parse_dates=['post_datetime'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b188ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality overview\n",
    "display(df.dtypes)\n",
    "missing = df.isna().mean().sort_values(ascending=False).to_frame('missing_rate')\n",
    "display(missing.head(20))\n",
    "\n",
    "desc_num = df.select_dtypes(include=np.number).describe().T\n",
    "display(desc_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a242d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: posting volume over time\n",
    "by_month = df.set_index('post_datetime').resample('M').size()\n",
    "plt.figure()\n",
    "by_month.plot(title='Number of Posts per Month')\n",
    "plt.xlabel('Month'); plt.ylabel('Posts'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81656b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: categorical distributions (platform, post_type, language)\n",
    "for name in ['platform','post_type','language']:\n",
    "    plt.figure()\n",
    "    df[name].value_counts().head(10).plot(kind='bar', title=f'Distribution: {name}')\n",
    "    plt.xlabel(name); plt.ylabel('Count'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d37013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: target and relationships\n",
    "target = 'engagement_rate'\n",
    "\n",
    "plt.figure()\n",
    "df[target].hist(bins=50)\n",
    "plt.title('Distribution of engagement_rate')\n",
    "plt.xlabel('engagement_rate'); plt.ylabel('Frequency'); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df.plot.scatter(x='followers', y='impressions', alpha=0.25)\n",
    "plt.title('Followers vs Impressions'); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df.plot.scatter(x='impressions', y='likes', alpha=0.25)\n",
    "plt.title('Impressions vs Likes'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ddd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Time\n",
    "df_fe['hour'] = df_fe['post_datetime'].dt.hour\n",
    "df_fe['dow'] = df_fe['post_datetime'].dt.dayofweek\n",
    "df_fe['month'] = df_fe['post_datetime'].dt.month\n",
    "df_fe['year'] = df_fe['post_datetime'].dt.year\n",
    "\n",
    "# Text\n",
    "df_fe['content_len'] = df_fe['content'].fillna('').str.len()\n",
    "df_fe['word_count'] = df_fe['content'].fillna('').str.split().apply(len)\n",
    "df_fe['hashtag_count'] = df_fe['hashtags'].fillna('').apply(lambda s: 0 if (pd.isna(s) or s == '') else s.count('#'))\n",
    "df_fe['has_link'] = df_fe['content'].fillna('').str.contains('http').astype(int)\n",
    "\n",
    "# Ratios & interaction rates\n",
    "df_fe['follow_ratio'] = (df_fe['followers']+1) / (df_fe['following']+1)\n",
    "df_fe['likes_per_impression'] = (df_fe['likes']+1) / (df_fe['impressions']+1)\n",
    "df_fe['comments_per_impression'] = (df_fe['comments']+1) / (df_fe['impressions']+1)\n",
    "df_fe['shares_per_impression'] = (df_fe['shares']+1) / (df_fe['impressions']+1)\n",
    "\n",
    "target = 'engagement_rate'\n",
    "y = df_fe[target]\n",
    "\n",
    "num_cols = [\n",
    "    'followers','following','impressions','likes','comments','shares',\n",
    "    'hour','dow','month','year','content_len','word_count','hashtag_count',\n",
    "    'has_link','follow_ratio','likes_per_impression','comments_per_impression','shares_per_impression'\n",
    "]\n",
    "cat_cols = ['platform','post_type','language','device','country','city','sentiment','topic','is_verified']\n",
    "\n",
    "X = df_fe[num_cols + cat_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler(with_mean=False))])\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "\n",
    "preprocessor_sparse = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "to_dense = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)  # for dense-only models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "model_specs = [\n",
    "    # name, estimator, needs_dense\n",
    "    ('SGD-L2 (Ridge-like)', SGDRegressor(loss='squared_error', penalty='l2', alpha=1e-4, max_iter=2000, random_state=RANDOM_STATE), False),\n",
    "    ('SGD-ElasticNet', SGDRegressor(loss='squared_error', penalty='elasticnet', l1_ratio=0.3, alpha=1e-4, max_iter=3000, random_state=RANDOM_STATE), False),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1), True),\n",
    "    ('GradientBoosting', GradientBoostingRegressor(random_state=RANDOM_STATE), True),\n",
    "    ('HistGradientBoosting', HistGradientBoostingRegressor(random_state=RANDOM_STATE), True),\n",
    "    ('DecisionTree', DecisionTreeRegressor(random_state=RANDOM_STATE), True),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "import pandas as pd\n",
    "\n",
    "scoring = {'MAE': 'neg_mean_absolute_error','RMSE': 'neg_root_mean_squared_error','R2':'r2'}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Optional: speed-up by subsampling for CV only (uncomment to use)\n",
    "# import numpy as np\n",
    "# sample_size = min(4000, X_train.shape[0])\n",
    "# sub_idx = np.random.RandomState(RANDOM_STATE).choice(np.arange(X_train.shape[0]), size=sample_size, replace=False)\n",
    "# X_cv, y_cv = X_train.iloc[sub_idx], y_train.iloc[sub_idx]\n",
    "# use_subset = True\n",
    "use_subset = False\n",
    "X_cv, y_cv = (X_train, y_train)\n",
    "\n",
    "rows = []\n",
    "for name, est, needs_dense in model_specs:\n",
    "    if needs_dense:\n",
    "        pipe = Pipeline([('prep', preprocessor_sparse), ('to_dense', to_dense), ('mdl', est)])\n",
    "    else:\n",
    "        pipe = Pipeline([('prep', preprocessor_sparse), ('mdl', est)])\n",
    "    scores = cross_validate(pipe, X_cv, y_cv, cv=cv, scoring=scoring, n_jobs=-1, return_estimator=False)\n",
    "    rows.append({\n",
    "        'model': name,\n",
    "        'MAE_mean': -scores['test_MAE'].mean(),\n",
    "        'MAE_std':  scores['test_MAE'].std(),\n",
    "        'RMSE_mean': -scores['test_RMSE'].mean(),\n",
    "        'RMSE_std':  scores['test_RMSE'].std(),\n",
    "        'R2_mean':  scores['test_R2'].mean(),\n",
    "        'R2_std':   scores['test_R2'].std(),\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(rows).sort_values('RMSE_mean').reset_index(drop=True)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on full training set; evaluate on test\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-9, None))) * 100\n",
    "\n",
    "best_name = cv_df.iloc[0]['model']\n",
    "best_row = cv_df.iloc[0].to_dict()\n",
    "print('Best by CV:', best_name)\n",
    "print(best_row)\n",
    "\n",
    "# Pull estimator and whether it needs dense\n",
    "spec_map = {name: (est, dense) for name, est, dense in model_specs}\n",
    "best_est, needs_dense = spec_map[best_name]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "if needs_dense:\n",
    "    best_pipe = Pipeline([('prep', preprocessor_sparse), ('to_dense', to_dense), ('mdl', best_est)])\n",
    "else:\n",
    "    best_pipe = Pipeline([('prep', preprocessor_sparse), ('mdl', best_est)])\n",
    "\n",
    "best_pipe.fit(X_train, y_train)\n",
    "preds = best_pipe.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "r2 = r2_score(y_test, preds)\n",
    "mape_val = mape(y_test, preds)\n",
    "\n",
    "print('Test MAE:', round(mae, 6))\n",
    "print('Test RMSE:', round(rmse, 6))\n",
    "print('Test R2:', round(r2, 6))\n",
    "print('Test MAPE (%):', round(mape_val, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals & actual-vs-predicted\n",
    "plt.figure()\n",
    "plt.scatter(preds, (y_test - preds), alpha=0.3)\n",
    "plt.axhline(0)\n",
    "plt.title('Residuals vs Predictions')\n",
    "plt.xlabel('Predicted engagement_rate'); plt.ylabel('Residual'); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, preds, alpha=0.3)\n",
    "plt.title('Actual vs Predicted engagement_rate')\n",
    "plt.xlabel('Actual'); plt.ylabel('Predicted'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (tree models only)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "is_tree = hasattr(best_pipe.named_steps['mdl'], 'feature_importances_')\n",
    "if is_tree:\n",
    "    # Refit to ensure attributes exist\n",
    "    best_pipe.fit(X_train, y_train)\n",
    "    # Extract transformed feature names\n",
    "    ohe = best_pipe.named_steps['prep'].named_transformers_['cat']\n",
    "    num_features = num_cols\n",
    "    cat_features = list(ohe.get_feature_names_out(cat_cols))\n",
    "    feature_names = num_features + cat_features\n",
    "\n",
    "    importances = getattr(best_pipe.named_steps['mdl'], 'feature_importances_', None)\n",
    "    if importances is not None:\n",
    "        importances = np.array(importances)\n",
    "        idx = np.argsort(importances)[-25:][::-1]\n",
    "        top = [(feature_names[i], importances[i]) for i in idx]\n",
    "        imp_df = pd.DataFrame(top, columns=['feature','importance'])\n",
    "        display(imp_df)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.barh(imp_df['feature'][::-1], imp_df['importance'][::-1])\n",
    "        plt.title('Top 25 Feature Importances'); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print('Best model is not tree-based; skipping built-in feature importances.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd200da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights\n",
    "hourly = df.groupby(df['post_datetime'].dt.hour)['engagement_rate'].mean()\n",
    "plt.figure(); hourly.plot(title='Average Engagement Rate by Hour'); plt.xlabel('Hour'); plt.ylabel('Avg Engagement Rate'); plt.show()\n",
    "\n",
    "top_platform = df.groupby('platform')['engagement_rate'].mean().sort_values(ascending=False).head(5)\n",
    "top_post_type = df.groupby('post_type')['engagement_rate'].mean().sort_values(ascending=False).head(5)\n",
    "display(top_platform.to_frame('avg_engagement_rate'))\n",
    "display(top_post_type.to_frame('avg_engagement_rate'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cd99c",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "- We engineered strong behavioral and content features and compared diverse models under cross-validation.\n",
    "- The best model (see leaderboard) is evaluated on a hold-out test set with MAE/RMSE/R²/MAPE.\n",
    "- Use importances and hourly/platform/post-type views to inform posting strategy.\n",
    "\n",
    "**Next steps**\n",
    "- Hyperparameter tuning (RandomizedSearchCV/Bayesian).\n",
    "- Richer text features (TF–IDF, transformer embeddings).\n",
    "- Time-based CV to respect temporal leakage and evaluate robustness to drift.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
